#!/usr/bin/env python

import argparse
import multiprocessing
import signal
import sys
from urllib.error import HTTPError
from urllib.request import urlopen, Request

from bs4 import BeautifulSoup


def fetch(url, silent=True, read=False):
    request = Request(url, method='GET')
    request.add_header('User-Agent', 'Sitemap fetch')

    try:
        response = urlopen(request)
    except HTTPError as error:
        sys.stderr.write(f'{error.code} {error.msg} {url}\n')
        if not silent:
            exit(1)
    else:
        if response:
            sys.stdout.write(f'{response.status} {response.msg} {response.url}\n')
        if read:
            return response.read().decode()
        else:
            return True


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Post check website.')
    parser.add_argument('url', help='URL to check')
    parser.add_argument(
        '-p', '--processes',
        help='Concurrent connections', type=int,
        metavar="processes", default=4)

    options = parser.parse_args()

    response = fetch(options.url, silent=False, read=True)

    root = BeautifulSoup(response, 'html.parser')
    loc_tags = root.select('url loc')
    sys.stdout.write(f'Found {len(loc_tags)} urls\n')

    original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
    pool = multiprocessing.Pool(processes=options.processes)
    signal.signal(signal.SIGINT, original_sigint_handler)

    try:
        pool.map_async(fetch, [loc.text for loc in loc_tags]).get()
    except KeyboardInterrupt:
        pool.terminate()
    else:
        pool.close()

    pool.join()
