#!/usr/bin/env python

import argparse
import multiprocessing
import signal
import sys
from urllib2 import urlopen, HTTPError, Request, URLError

from bs4 import BeautifulSoup


def fetch(url, silent=True, read=False):
    request = Request(url)
    request.add_header('User-Agent', 'Sitemap fetch')

    try:
        response = urlopen(request)
    except (URLError, HTTPError) as error:
        sys.stderr.write('{error} {url}\n'.format(error=error, url=url))
        if not silent:
            exit(1)
    else:
        if response:
            sys.stdout.write('{status} {url}\n'.format(
                status=response.getcode(),
                url=response.geturl()
            ))
        if read:
            return response.read().decode()
        else:
            return True


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Post check website.')
    parser.add_argument('url', help='URL to check')
    parser.add_argument(
        '-p', '--processes',
        help='Concurrent connections', type=int,
        metavar="processes", default=4)

    options = parser.parse_args()

    response = fetch(options.url, silent=False, read=True)

    root = BeautifulSoup(response, 'html.parser')
    loc_tags = root.select('url loc')
    sys.stdout.write('Found {0} urls\n'.format(len(loc_tags)))

    original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
    pool = multiprocessing.Pool(processes=options.processes)
    signal.signal(signal.SIGINT, original_sigint_handler)

    try:
        pool.map_async(fetch, [loc.text for loc in loc_tags]).get()
    except KeyboardInterrupt:
        pool.terminate()
    else:
        pool.close()

    pool.join()
